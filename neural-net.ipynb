{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "This material is based on the Machine learning class in https://github.com/jyurko/INFSCI_2595_Fall_2019\n",
    "\n",
    "A neural network is a series of functional transformations, where the ouput is modeled via a set of unobserved variables called hidden units.\n",
    "\n",
    "The hidden units are linear combinations of inputs that are transformed into a non-linear function.\n",
    "\n",
    "Hidden units:\n",
    "\n",
    "$$\\beta_{0,1} + \\sum_{d=1}^{D}x_d\\beta_{d,1}$$\n",
    "\n",
    "pass through a non-linear function $g$\n",
    "\n",
    "$$h_1 = g(\\beta_{0,1} + \\sum_{d=1}^{D}x_d\\beta_{d,1})$$\n",
    "\n",
    "where D is the number of inputs ($x_1, x_2, ..., x_D$). The parameters $\\beta_{d,h}$ are also known as the weights, and $\\beta_{0,h}$ as the bias of each hidden unit $h$, also commonly called as neurons.\n",
    "\n",
    "The output $f$ is a linear combination of the hidden units\n",
    "\n",
    "$$f(\\mathbf{x}) = \\alpha_0 + \\sum_{k=1}^{K} \\alpha_kh_k$$\n",
    "\n",
    "The design matrix $\\mathbf{X}$ is of size $N \\times (D+1)$, where N is the number of training points. Each column is a variable plus the intercept column of ones.\n",
    "\n",
    "The linear combination of the inputs for the k-th hidden unit:\n",
    "\n",
    "$$\\eta_k = \\mathbf{X}\\boldsymbol\\beta_k$$\n",
    "\n",
    "The non-linear transformation function is also called activation function. Common functions might be the relu function, or the sigmoid function. We will use the logistic function, which is a type of sigmoid:\n",
    "\n",
    "$$g(u) = \\frac{exp(u)}{exp(u)+1} = logit^{-1}(u)$$\n",
    "\n",
    "The k-th hidden unit can be written as\n",
    "\n",
    "$$h_k = logit^{-1}(\\eta_k)$$\n",
    "\n",
    "and in matrix form:\n",
    "\n",
    "$$\\mathbf{H} = logit^{-1}(\\mathbf{X}_{(N \\times (D+1))}\\mathbf{B}_{((D+1) \\times H)})$$\n",
    "\n",
    "And the output layer:\n",
    "\n",
    "$$\\mathbf{f} = \\alpha_0 + H_{(H \\times 1)} \\boldsymbol \\alpha_{(1 \\times H)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "        class characterizing a layer and its processes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(1, output_size)\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.eta = None\n",
    "\n",
    "    def activation(x):\n",
    "        g = np.exp(x) / (np.exp(x) + 1)\n",
    "        return g\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.eta = np.dot(self.input, self.weights) + self.bias\n",
    "        self.output = activation(self.eta)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        self.step_size = output_error * learning_rate\n",
    "        self.weights = self.weights - self.step_size\n",
    "        self.bias = self.bias - self.step_size\n",
    "        return self.weights, self.bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "dat = np.random.rand(1,4)\n",
    "y = np.square(dat)\n",
    "\n",
    "H = Layer(input_size=4, output_size=4)\n",
    "\n",
    "output = Layer.forward_propagation(H, input_data=dat)\n",
    "\n",
    "error = np.mean(np.square(y - output))\n",
    "de_dy = (2/4)*(y - output)\n",
    "\n",
    "new_weight, new_bias = Layer.backward_propagation(H, output_error=de_dy, learning_rate=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.73589269, 0.45885402, 1.02162361, 0.71227537],\n       [0.49735563, 0.42786508, 0.38403743, 0.75649534],\n       [0.45499597, 0.09542546, 0.43890367, 0.76544104],\n       [0.19891546, 0.21119932, 0.57241079, 0.55927322]])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.65082468, 0.88517936, 0.76531474, 0.63846914]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.9012055 , 0.86557953, 0.94113844, 0.95085632]])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.weights = new_weight\n",
    "H.bias = new_bias\n",
    "\n",
    "output2 = Layer.forward_propagation(H, input_data=output)\n",
    "output2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.20806809, -0.39185191, -0.44483843, -0.32345417]])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error2 = np.mean(np.square(y - output2))\n",
    "de_dy2 = (2/4)*(y - output2)\n",
    "de_dy2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.04383084, 0.03437626, 0.03624431, 0.04899785]])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_dy - de_dy2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}